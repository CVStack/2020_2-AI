{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PytorchTensor2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT239HZosPiV2ygq8CgIv2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CVStack/2020_2-AI/blob/master/PytorchTensor2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5h1MJsgWS83",
        "outputId": "d903b76e-03a5-4281-8798-83d7f11d85d7"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        " = Variable(torch.tensor([[2.]]), requires_grad = True)\n",
        "print('x = ', x)\n",
        "print('x.data = ', x.data) #x data 깂\n",
        "print('x.grad = ', x.grad) #x gradient 값\n",
        "print('x.grad_fn() = ', x.grad_fn)\n",
        "\n",
        "y = x * x * 3\n",
        "print('\\ny = ', y)\n",
        "print('y.data = ', y.data)\n",
        "print('y.grad = ', y.grad)\n",
        "print('y.grad_fn() = ', y.grad_fn)\n",
        "\n",
        "z = y**2\n",
        "print('\\nz = ', z)\n",
        "print('z.data = ', z.data)\n",
        "print('z.grad = ', z.grad)\n",
        "\n",
        "z.backward( ) #backpropagation\n",
        "\n",
        "# z를 변수 x에 대해서 미분한 값을 구함 --> dz / dx를 구할 수 있음.\n",
        "print('\\nAfter invocation of backward()')\n",
        "print('\\nx = ', x)\n",
        "print('x.data = ', x.data)\n",
        "print('x.grad = ', x.grad) #dz / dx = (dz / dy) * (dy / dx)\n",
        "print('x.grad_fn( ) = ', x.grad_fn)\n",
        "print('\\ny = ', y)\n",
        "print('y.data = ', y.data)\n",
        "print('y.grad = ', y.grad)\n",
        "print('y.grad_fn( ) = ', y.grad_fn)\n",
        "print('\\nz = ', z)\n",
        "print('z.data = ', z.data)\n",
        "print('z.grad = ', z.grad)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "y =  tensor([[2.]], requires_grad=True)\n",
            "y.data =  tensor([[2.]])\n",
            "y.grad =  None\n",
            "y.grad_fn() =  None\n",
            "\n",
            "z =  tensor([[4.]], grad_fn=<PowBackward0>)\n",
            "z.data =  tensor([[4.]])\n",
            "z.grad =  None\n",
            "\n",
            "After invocation of backward()\n",
            "\n",
            "y =  tensor([[2.]], requires_grad=True)\n",
            "y.data =  tensor([[2.]])\n",
            "y.grad =  tensor([[4.]])\n",
            "y.grad_fn( ) =  None\n",
            "\n",
            "z =  tensor([[4.]], grad_fn=<PowBackward0>)\n",
            "z.data =  tensor([[4.]])\n",
            "z.grad =  None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmJK8fOZXbXC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}